import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
import joblib
import warnings
import re
import gc
from tqdm import tqdm
from fuzzywuzzy import fuzz, process
import optuna
from geopy.geocoders import Nominatim
from geopy.distance import geodesic
import psutil
import os
warnings.filterwarnings('ignore')


class MemoryManager:
    """16GB RAM iÃ§in memory yÃ¶netimi"""
    
    @staticmethod
    def get_memory_usage():
        """Mevcut memory kullanÄ±mÄ±nÄ± kontrol et"""
        memory = psutil.virtual_memory()
        return {
            'total': memory.total / (1024**3),  # GB
            'available': memory.available / (1024**3),  # GB
            'used': memory.used / (1024**3),  # GB
            'percent': memory.percent
        }
    
    @staticmethod
    def check_memory_safe():
        """Memory gÃ¼venli mi kontrol et"""
        memory_info = MemoryManager.get_memory_usage()
        print(f"ðŸ’¾ Memory KullanÄ±mÄ±: {memory_info['used']:.1f}GB / {memory_info['total']:.1f}GB ({memory_info['percent']:.1f}%)")
        
        if memory_info['available'] < 2.0:  # 2GB'den az kaldÄ±ysa
            print("âš ï¸ DÃ¼ÅŸÃ¼k memory! Garbage collection yapÄ±lÄ±yor...")
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            return False
        return True
    
    @staticmethod
    def optimize_batch_size(embedding_dim, num_samples):
        """16GB RAM iÃ§in optimal batch size hesapla"""
        available_memory = MemoryManager.get_memory_usage()['available']
        
        # Her embedding iÃ§in yaklaÅŸÄ±k memory kullanÄ±mÄ± (float32)
        memory_per_sample = embedding_dim * 4 / (1024**3)  # GB
        
        # GÃ¼venli batch size (available memory'nin %70'ini kullan)
        safe_memory = available_memory * 0.7
        optimal_batch_size = int(safe_memory / memory_per_sample)
        
        # SÄ±nÄ±rlar
        optimal_batch_size = max(16, min(optimal_batch_size, 512))
        
        print(f" Optimal batch size: {optimal_batch_size} (embedding_dim: {embedding_dim})")
        return optimal_batch_size

class CheckpointManager:
    """Checkpoint sistemi - Embedding ve model durumlarÄ±nÄ± kaydet/yÃ¼kle"""
    
    def __init__(self, checkpoint_dir='checkpoints'):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        
    def _get_checkpoint_path(self, checkpoint_name):
        """Checkpoint dosya yolu oluÅŸtur"""
        return os.path.join(self.checkpoint_dir, f"{checkpoint_name}.pkl")
    
    def _get_metadata_path(self, checkpoint_name):
        """Metadata dosya yolu oluÅŸtur"""
        return os.path.join(self.checkpoint_dir, f"{checkpoint_name}_metadata.json")
    
    def save_checkpoint(self, checkpoint_name, data, metadata=None):
        """Checkpoint kaydet"""
        checkpoint_path = self._get_checkpoint_path(checkpoint_name)
        metadata_path = self._get_metadata_path(checkpoint_name)
        
        try:
            # Ana veriyi kaydet
            joblib.dump(data, checkpoint_path)
            
            # Metadata kaydet
            if metadata is None:
                metadata = {}
            metadata['timestamp'] = pd.Timestamp.now().isoformat()
            metadata['checkpoint_name'] = checkpoint_name
            
            import json
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… Checkpoint kaydedildi: {checkpoint_path}")
            return True
            
        except Exception as e:
            print(f"âŒ Checkpoint kaydetme hatasÄ±: {e}")
            return False
    
    def load_checkpoint(self, checkpoint_name):
        """Checkpoint yÃ¼kle"""
        checkpoint_path = self._get_checkpoint_path(checkpoint_name)
        metadata_path = self._get_metadata_path(checkpoint_name)
        
        if not os.path.exists(checkpoint_path):
            print(f"âš ï¸ Checkpoint bulunamadÄ±: {checkpoint_path}")
            return None, None
        
        try:
            # Ana veriyi yÃ¼kle
            data = joblib.load(checkpoint_path)
            
            # Metadata yÃ¼kle
            metadata = None
            if os.path.exists(metadata_path):
                import json
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
            
            print(f"âœ… Checkpoint yÃ¼klendi: {checkpoint_path}")
            if metadata:
                print(f"ðŸ“… OluÅŸturulma: {metadata.get('timestamp', 'Bilinmiyor')}")
            
            return data, metadata
            
        except Exception as e:
            print(f"âŒ Checkpoint yÃ¼kleme hatasÄ±: {e}")
            return None, None
    
    def checkpoint_exists(self, checkpoint_name):
        """Checkpoint var mÄ± kontrol et"""
        checkpoint_path = self._get_checkpoint_path(checkpoint_name)
        return os.path.exists(checkpoint_path)
    
    def get_checkpoint_info(self, checkpoint_name):
        """Checkpoint bilgilerini al"""
        metadata_path = self._get_metadata_path(checkpoint_name)
        if os.path.exists(metadata_path):
            import json
            with open(metadata_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return None
    
    def list_checkpoints(self):
        """Mevcut checkpoint'leri listele"""
        checkpoints = []
        for file in os.listdir(self.checkpoint_dir):
            if file.endswith('.pkl') and not file.endswith('_metadata.json'):
                checkpoint_name = file.replace('.pkl', '')
                metadata = self.get_checkpoint_info(checkpoint_name)
                checkpoints.append({
                    'name': checkpoint_name,
                    'metadata': metadata
                })
        return checkpoints

class DistilledAddressModel(nn.Module):
    """Distilled (kÃ¼Ã§Ã¼ltÃ¼lmÃ¼ÅŸ) adres modeli"""
    
    def __init__(self, embedding_dim=384, num_labels=10390, hidden_dims=[256, 128]):
        super().__init__()
        
        # Daha kÃ¼Ã§Ã¼k embedding boyutu
        self.embedding_dim = embedding_dim
        
        # Hafif classifier
        layers = []
        input_dim = embedding_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(input_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2)  # Daha az dropout
            ])
            input_dim = hidden_dim
        
        layers.append(nn.Linear(input_dim, num_labels))
        
        self.classifier = nn.Sequential(*layers)
        
    def forward(self, embeddings):
        return self.classifier(embeddings)

class UnifiedLoss(nn.Module):
    """TÃ¼m optimizasyonlarÄ± birleÅŸtiren unified loss function"""
    
    def __init__(self, temperature=4.0, alpha=0.7):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha
        
        # Loss functions
        self.ce_loss = nn.CrossEntropyLoss()
        self.kl_loss = nn.KLDivLoss(reduction='batchmean')
        self.contrastive_loss = nn.CosineEmbeddingLoss()
        
        # Adaptive loss weights
        self.loss_weights = {
            'main': 1.0,
            'region': 0.1,      # Azalt
            'type': 0.1,        # Azalt
            'distillation': 0.3, # Azalt
            'similarity': 0.1,   # Azalt
            'l2_reg': 0.001     # Ã‡ok azalt
        }
        
    def _create_meaningful_auxiliary_labels(self, targets, batch_size):
        """GerÃ§ekÃ§i auxiliary labels oluÅŸtur"""
        # Region labels: Label ID'sine gÃ¶re region belirle
        region_labels = (targets % 10).to(targets.device)  # 0-9 arasÄ±
        
        # Type labels: Label ID'sine gÃ¶re type belirle  
        type_labels = (targets % 5).to(targets.device)   # 0-4 arasÄ±
        
        return region_labels, type_labels
        
    def forward(self, outputs, targets, teacher_logits=None, similarity_pairs=None):
        """
        Unified loss calculation
        outputs: Unified model outputs
        targets: Main classification targets
        teacher_logits: Teacher model logits (for distillation)
        similarity_pairs: Positive/negative pairs (for contrastive learning)
        """
        total_loss = 0
        loss_components = {}
        
        # 1. Main classification loss (en Ã¶nemli)
        if 'main' in outputs:
            main_loss = self.ce_loss(outputs['main'], targets)
            total_loss += self.loss_weights['main'] * main_loss
            loss_components['main'] = main_loss.item()
        
        # 2. Hierarchical classification loss (region) - anlamlÄ± labels
        if 'region' in outputs:
            region_labels, _ = self._create_meaningful_auxiliary_labels(targets, targets.size(0))
            region_loss = self.ce_loss(outputs['region'], region_labels)
            total_loss += self.loss_weights['region'] * region_loss
            loss_components['region'] = region_loss.item()
        
        # 3. Address type classification loss - anlamlÄ± labels
        if 'type' in outputs:
            _, type_labels = self._create_meaningful_auxiliary_labels(targets, targets.size(0))
            type_loss = self.ce_loss(outputs['type'], type_labels)
            total_loss += self.loss_weights['type'] * type_loss
            loss_components['type'] = type_loss.item()
        
        # 4. Knowledge distillation loss
        if 'distillation' in outputs and teacher_logits is not None:
            student_log_softmax = F.log_softmax(outputs['distillation'] / self.temperature, dim=1)
            teacher_softmax = F.softmax(teacher_logits / self.temperature, dim=1)
            distillation_loss = self.kl_loss(student_log_softmax, teacher_softmax) * (self.temperature ** 2)
            total_loss += self.loss_weights['distillation'] * distillation_loss
            loss_components['distillation'] = distillation_loss.item()
        
        # 5. Contrastive learning loss (similarity)
        if 'similarity' in outputs and similarity_pairs is not None:
            (pos_emb_a, pos_emb_b), (neg_emb_a, neg_emb_b) = similarity_pairs
            
            # Positive pairs iÃ§in contrastive loss
            if pos_emb_a.size(0) > 0:
                pos_loss = self.contrastive_loss(pos_emb_a, pos_emb_b, torch.ones(pos_emb_a.size(0), device=targets.device))
            else:
                pos_loss = torch.tensor(0.0, device=targets.device)
            
            # Negative pairs iÃ§in contrastive loss
            if neg_emb_a.size(0) > 0:
                neg_loss = self.contrastive_loss(neg_emb_a, neg_emb_b, torch.ones(neg_emb_a.size(0), device=targets.device) * -1)
            else:
                neg_loss = torch.tensor(0.0, device=targets.device)
            
            similarity_loss = pos_loss + neg_loss
            total_loss += self.loss_weights['similarity'] * similarity_loss
            loss_components['similarity'] = similarity_loss.item()
        
        # 6. Feature regularization (L2 penalty)
        if 'shared_features' in outputs:
            l2_reg = torch.norm(outputs['shared_features'], p=2, dim=1).mean()
            total_loss += self.loss_weights['l2_reg'] * l2_reg
            loss_components['l2_reg'] = l2_reg.item()
        
        return total_loss, loss_components

class MemoryEfficientEmbeddingModel:
    """16GB RAM iÃ§in optimize edilmiÅŸ embedding modeli - CHECKPOINT SÄ°STEMÄ° Ä°LE"""
    
    def __init__(self, model_name="sentence-transformers/all-MiniLM-L6-v2", device=None):
        self.model = SentenceTransformer(model_name)
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        # Checkpoint sistemi
        self.checkpoint_manager = CheckpointManager()
        
        # Memory kontrolÃ¼
        MemoryManager.check_memory_safe()
    
    def get_embeddings_memory_efficient(self, addresses, batch_size=None, use_checkpoints=True):
        """Memory-efficient embedding hesaplama - CHECKPOINT SÄ°STEMÄ° Ä°LE"""
        print("ðŸ“ Memory-efficient embedding hesaplanÄ±yor...")
        
        # Checkpoint kontrolÃ¼
        if use_checkpoints:
            embedding_checkpoint = f"embeddings_{len(addresses)}_{hash(str(addresses[:5]))}"
            
            if self.checkpoint_manager.checkpoint_exists(embedding_checkpoint):
                print("ðŸ“‚ Embedding checkpoint bulundu, yÃ¼kleniyor...")
                embedding_data, metadata = self.checkpoint_manager.load_checkpoint(embedding_checkpoint)
                
                if embedding_data is not None:
                    print(f"âœ… Embedding'ler checkpoint'ten yÃ¼klendi: {embedding_data['embeddings'].shape}")
                    return embedding_data['embeddings']
        
        # Yeni embedding hesaplama
        if batch_size is None:
            batch_size = MemoryManager.optimize_batch_size(384, len(addresses))
        
        all_embeddings = []
        
        for i in tqdm(range(0, len(addresses), batch_size), desc="Embedding calculation"):
            # Memory kontrolÃ¼
            if not MemoryManager.check_memory_safe():
                print("âš ï¸ Memory kritik seviyede, batch size kÃ¼Ã§Ã¼ltÃ¼lÃ¼yor...")
                batch_size = max(8, batch_size // 2)
            
            batch = addresses[i:i+batch_size]
            embeddings = self.model.encode(batch, show_progress_bar=False, batch_size=batch_size)
            all_embeddings.append(embeddings)
            
            # Memory temizleme
            if i % (batch_size * 10) == 0:
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        
        final_embeddings = np.vstack(all_embeddings)
        
        # Embedding checkpoint kaydet
        if use_checkpoints:
            embedding_checkpoint = f"embeddings_{len(addresses)}_{hash(str(addresses[:5]))}"
            embedding_data = {
                'embeddings': final_embeddings,
                'addresses': addresses,
                'model_name': self.model.get_sentence_embedding_dimension()
            }
            metadata = {
                'data_size': len(addresses),
                'embedding_shape': final_embeddings.shape,
                'model_name': str(self.model)
            }
            self.checkpoint_manager.save_checkpoint(embedding_checkpoint, embedding_data, metadata)
        
        return final_embeddings

class HierarchicalAddressClassifier:
    """Ä°ki aÅŸamalÄ± hierarchical sÄ±nÄ±flandÄ±rma"""
    
    def __init__(self):
        self.region_classifier = None  # CoÄŸrafi bÃ¶lge sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±
        self.specific_classifier = None  # Spesifik adres sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±
        self.region_mapping = {}  # BÃ¶lge -> spesifik adresler mapping
    
    def extract_region_features(self, address):
        """CoÄŸrafi bÃ¶lge Ã¶zelliklerini Ã§Ä±kar"""
        # TÃ¼rkÃ§e ÅŸehir/ilÃ§e kalÄ±plarÄ±
        region_patterns = [
            r'\b(istanbul|ankara|izmir|bursa|antalya|adana|konya|gaziantep|mersin|diyarbakir)\b',
            r'\b(beyoglu|kadikoy|sisli|uskudar|fatih|besiktas|sariyer|maltepe|pendik|kartal)\b',
            r'\b(merkez|ilce|mahalle|semt|bolge|cevre|yakin)\b'
        ]
        
        features = {}
        for i, pattern in enumerate(region_patterns):
            features[f'region_pattern_{i}'] = 1 if re.search(pattern, address.lower()) else 0
        
        return features
    
    def create_hierarchical_structure(self, train_df):
        """Hierarchical yapÄ± oluÅŸtur"""
        print("ðŸ—ï¸ Hierarchical yapÄ± oluÅŸturuluyor...")
        
        # CoÄŸrafi bÃ¶lgeleri belirle
        regions = []
        for address in train_df['address']:
            region_features = self.extract_region_features(address)
            # Basit region belirleme (gerÃ§ek uygulamada daha geliÅŸmiÅŸ olabilir)
            if any(region_features.values()):
                regions.append('urban')
            else:
                regions.append('rural')
        
        train_df['region'] = regions
        
        # Region mapping oluÅŸtur
        for region in train_df['region'].unique():
            region_data = train_df[train_df['region'] == region]
            self.region_mapping[region] = region_data['label'].unique().tolist()
        
        print(f"âœ… Hierarchical yapÄ±: {len(self.region_mapping)} bÃ¶lge")
        return train_df

class AdvancedDataAugmentation:
    """GeliÅŸmiÅŸ veri augmentation - CHECKPOINT SÄ°STEMÄ° Ä°LE"""
    
    def __init__(self):
        self.turkish_patterns = {
            'mahalle': ['mahalle', 'mah', 'mh.', 'mh'],
            'sokak': ['sokak', 'sok', 'sk.', 'sk'],
            'cadde': ['cadde', 'cad', 'c.', 'cd'],
            'bulvar': ['bulvar', 'bulv', 'blv.', 'blv'],
            'apartman': ['apartman', 'apt.', 'apt'],
            'kat': ['kat', 'k.', 'floor'],
            'numara': ['numara', 'no', 'nr.', 'num']
        }
        
        # Checkpoint sistemi
        self.checkpoint_manager = CheckpointManager()
    
    def augment_address(self, address):
        """Tek adres iÃ§in augmentation"""
        variations = [address]
        
        # TÃ¼rkÃ§e karakter varyasyonlarÄ±
        char_variations = [
            address.replace('Ã§', 'c').replace('ÄŸ', 'g').replace('Ä±', 'i').replace('Ã¶', 'o').replace('ÅŸ', 's').replace('Ã¼', 'u'),
            address.replace('Ã‡', 'C').replace('Äž', 'G').replace('I', 'I').replace('Ä°', 'I').replace('Ã–', 'O').replace('Åž', 'S').replace('Ãœ', 'U')
        ]
        variations.extend(char_variations)
        
        # KÄ±saltma varyasyonlarÄ±
        for full, shorts in self.turkish_patterns.items():
            for short in shorts[1:]:  # Ä°lk eleman orijinal
                if full in address.lower():
                    variations.append(address.lower().replace(full, short))
        
        # SayÄ± format varyasyonlarÄ±
        number_patterns = [
            (r'No:(\d+)', r'No \1'),
            (r'No:(\d+)', r'No. \1'),
            (r'(\d+)\s*kat', r'kat \1'),
            (r'(\d+)\s*daire', r'daire \1')
        ]
        
        for pattern, replacement in number_patterns:
            if re.search(pattern, address):
                variations.append(re.sub(pattern, replacement, address))
        
        return list(set(variations))  # Duplicate'larÄ± kaldÄ±r
    
    def augment_dataset(self, train_df, augmentation_factor=2, use_checkpoints=True):
        """TÃ¼m dataset iÃ§in augmentation - CHECKPOINT SÄ°STEMÄ° Ä°LE"""
        print(f"ðŸ”„ Dataset augmentation baÅŸlÄ±yor (factor: {augmentation_factor})...")
        
        # Checkpoint kontrolÃ¼
        if use_checkpoints:
            augmentation_checkpoint = f"augmentation_{len(train_df)}_{augmentation_factor}_{hash(str(train_df['address'].iloc[:5].tolist()))}"
            
            if self.checkpoint_manager.checkpoint_exists(augmentation_checkpoint):
                print("ðŸ“‚ Augmentation checkpoint bulundu, yÃ¼kleniyor...")
                augmentation_data, metadata = self.checkpoint_manager.load_checkpoint(augmentation_checkpoint)
                
                if augmentation_data is not None:
                    augmented_df = augmentation_data['augmented_df']
                    print(f"âœ… Augmentation checkpoint'ten yÃ¼klendi: {len(train_df)} â†’ {len(augmented_df)}")
                    return augmented_df
        
        # Yeni augmentation
        augmented_data = []
        
        for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc="Augmentation"):
            # Orijinal veri
            augmented_data.append({
                'address': row['address'],
                'label': row['label']
            })
            
            # Augmented veriler
            variations = self.augment_address(row['address'])
            for variation in variations[:augmentation_factor-1]:  # Orijinal hariÃ§
                if variation != row['address']:  # AynÄ± deÄŸilse
                    augmented_data.append({
                        'address': variation,
                        'label': row['label']
                    })
        
        augmented_df = pd.DataFrame(augmented_data)
        print(f"âœ… Augmentation tamamlandÄ±: {len(train_df)} â†’ {len(augmented_df)}")
        
        # Augmentation checkpoint kaydet
        if use_checkpoints:
            augmentation_checkpoint = f"augmentation_{len(train_df)}_{augmentation_factor}_{hash(str(train_df['address'].iloc[:5].tolist()))}"
            augmentation_data = {
                'augmented_df': augmented_df,
                'original_size': len(train_df),
                'augmented_size': len(augmented_df),
                'augmentation_factor': augmentation_factor
            }
            metadata = {
                'original_size': len(train_df),
                'augmented_size': len(augmented_df),
                'augmentation_factor': augmentation_factor
            }
            self.checkpoint_manager.save_checkpoint(augmentation_checkpoint, augmentation_data, metadata)
        
        return augmented_df

class UnifiedAddressModel(nn.Module):
    """TÃ¼m optimizasyonlarÄ± birleÅŸtiren unified model"""
    
    def __init__(self, embedding_dim=384, num_labels=10390):
        super().__init__()
        
        # Shared feature extractor
        self.feature_extractor = nn.Sequential(
            nn.Linear(embedding_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        
        # Main classification head
        self.main_classifier = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_labels)
        )
        
        # Hierarchical classification head (bÃ¶lge tahmini)
        self.region_classifier = nn.Sequential(
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 10)  # 10 bÃ¶lge
        )
        
        # Address type classifier (ev, iÅŸ, vb.)
        self.type_classifier = nn.Sequential(
            nn.Linear(256, 32),
            nn.ReLU(),
            nn.Linear(32, 5)  # 5 tip
        )
        
        # Similarity head (contrastive learning iÃ§in)
        self.similarity_head = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 64)  # Similarity embedding
        )
        
        # Distillation head (knowledge distillation iÃ§in)
        self.distillation_head = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, num_labels)
        )
        
        # Sabit teacher weights (bir kez oluÅŸtur)
        self.teacher_weights = nn.Parameter(torch.randn(num_labels, embedding_dim))
        nn.init.xavier_uniform_(self.teacher_weights)  # DÃ¼zgÃ¼n initialization
        
        # DÃ¼zgÃ¼n initialization
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """DÃ¼zgÃ¼n weight initialization"""
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.BatchNorm1d):
            nn.init.ones_(module.weight)
            nn.init.zeros_(module.bias)
    
    def get_teacher_logits(self, embeddings):
        """Sabit teacher logits hesapla"""
        return F.linear(embeddings, self.teacher_weights)
        
    def forward(self, embeddings, task='main'):
        """Unified forward pass - tÃ¼m tasklar iÃ§in"""
        shared_features = self.feature_extractor(embeddings)
        
        if task == 'main':
            # Ana sÄ±nÄ±flandÄ±rma
            main_logits = self.main_classifier(shared_features)
            return main_logits
        
        elif task == 'all':
            # TÃ¼m tasklar birlikte
            main_logits = self.main_classifier(shared_features)
            region_logits = self.region_classifier(shared_features)
            type_logits = self.type_classifier(shared_features)
            similarity_emb = self.similarity_head(shared_features)
            distillation_logits = self.distillation_head(shared_features)
            
            return {
                'main': main_logits,
                'region': region_logits,
                'type': type_logits,
                'similarity': similarity_emb,
                'distillation': distillation_logits,
                'shared_features': shared_features
            }
        
        elif task == 'similarity':
            # Sadece similarity embedding
            return self.similarity_head(shared_features)
        
        elif task == 'distillation':
            # Sadece distillation
            return self.distillation_head(shared_features)
        
        else:
            raise ValueError(f"Bilinmeyen task: {task}")

class OptimizedBERTAddressMatcher:
    """16GB RAM iÃ§in optimize edilmiÅŸ BERT adres eÅŸleÅŸtirme - UNIFIED MODEL"""
    
    def __init__(self, device=None):
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.embedding_model = None
        self.unified_model = None  # Tek unified model
        self.preprocessor = None
        self.train_embeddings = None
        self.train_labels = None
        self.train_addresses = None
        self.label_encoder = None
        self.tfidf_vectorizer = None
        self.rf_classifier = None
        
        # Checkpoint sistemi
        self.checkpoint_manager = CheckpointManager()
        
        print(f"ðŸš€ UNIFIED BERT Address Matcher")
        print(f"ðŸ“± Cihaz: {self.device}")
        MemoryManager.check_memory_safe()
    
    def fit(self, train_df, use_augmentation=True, use_checkpoints=True):
        """UNIFIED MODEL EÄžÄ°TÄ°MÄ° - CHECKPOINT SÄ°STEMÄ° Ä°LE"""
        print("ðŸš€ UNIFIED BERT ADRES EÅžLEÅžTÄ°RME MODELÄ° EÄžÄ°TÄ°MÄ°")
        print("=" * 80)
        print("ðŸŽ¯ UNIFIED MODEL - TÃœM OPTÄ°MÄ°ZASYONLAR BÄ°RLEÅžTÄ°RÄ°LDÄ°:")
        print("  âœ… Memory Management (16GB RAM iÃ§in)")
        print("  âœ… Advanced Data Augmentation")
        print("  âœ… Multi-Task Learning (Main + Region + Type)")
        print("  âœ… Knowledge Distillation")
        print("  âœ… Contrastive Learning (Similarity)")
        print("  âœ… Feature Regularization")
        print("  âœ… Lightweight Ensemble")
        print("  âœ… Checkpoint Sistemi (GÃ¼venli kaydetme)")
        print("=" * 80)
        print(f"ðŸ“Š EÄŸitim verisi boyutu: {len(train_df)}")
        
        # Memory kontrolÃ¼
        MemoryManager.check_memory_safe()
        
        # CHECKPOINT KONTROLÃœ - TÃ¼m model durumu
        if use_checkpoints:
            print("\nðŸ” CHECKPOINT KONTROLÃœ")
            print("-" * 30)
            
            # Mevcut checkpoint'leri kontrol et
            checkpoints = self.checkpoint_manager.list_checkpoints()
            full_model_checkpoints = [cp for cp in checkpoints if cp['name'].startswith('full_model_')]
            
            if full_model_checkpoints:
                # En son checkpoint'i kullan
                latest_checkpoint = full_model_checkpoints[-1]['name']
                print(f"ðŸ“‚ Mevcut checkpoint bulundu: {latest_checkpoint}")
                model_data, metadata = self.checkpoint_manager.load_checkpoint(latest_checkpoint)
                
                if model_data is not None:
                    print("âœ… Checkpoint'ten model verileri yÃ¼klendi!")
                    
                    # TÃ¼m model bileÅŸenlerini yÃ¼kle
                    self.train_embeddings = model_data['train_embeddings']
                    self.train_addresses = model_data['train_addresses']
                    self.train_labels = model_data['train_labels']
                    self.label_encoder = model_data['label_encoder']
                    self.preprocessor = model_data['preprocessor']
                    self.tfidf_vectorizer = model_data.get('tfidf_vectorizer')
                    self.rf_classifier = model_data.get('rf_classifier')
                    
                    # Embedding model'i yeniden oluÅŸtur
                    self.embedding_model = MemoryEfficientEmbeddingModel(device=self.device)
                    
                    # Unified model'i yÃ¼kle
                    if 'unified_model_state' in model_data:
                        num_labels = len(self.label_encoder.classes_)
                        self.unified_model = UnifiedAddressModel(
                            embedding_dim=384,
                            num_labels=num_labels
                        ).to(self.device)
                        self.unified_model.load_state_dict(model_data['unified_model_state'])
                        self.unified_model.eval()
                        print("âœ… Unified model checkpoint'ten yÃ¼klendi")
                    
                    print(f"âœ… Tam model checkpoint'ten yÃ¼klendi: {self.train_embeddings.shape}")
                    print("ðŸ“Š YÃ¼klenen bileÅŸenler:")
                    print(f"  - Embeddings: {self.train_embeddings.shape}")
                    print(f"  - Labels: {len(self.train_labels)}")
                    print(f"  - Addresses: {len(self.train_addresses)}")
                    print(f"  - Unified Model: {'âœ…' if self.unified_model else 'âŒ'}")
                    print(f"  - TF-IDF: {'âœ…' if self.tfidf_vectorizer else 'âŒ'}")
                    print(f"  - Random Forest: {'âœ…' if self.rf_classifier else 'âŒ'}")
                    
                    print("\nâœ… UNIFIED model eÄŸitimi tamamlandÄ± (checkpoint'ten)!")
                    print("ðŸš€ Model kullanÄ±ma hazÄ±r!")
                    MemoryManager.check_memory_safe()
                    return
            else:
                print("âš ï¸ Mevcut checkpoint bulunamadÄ±, yeni eÄŸitim baÅŸlatÄ±lacak")
        
        # YENÄ° EÄžÄ°TÄ°M - Checkpoint yoksa
        print("ðŸ†• Yeni eÄŸitim baÅŸlatÄ±lÄ±yor...")
        
        # 1. Data Augmentation
        if use_augmentation:
            print("\nðŸ”„ ADVANCED DATA AUGMENTATION")
            print("-" * 40)
            augmentation = AdvancedDataAugmentation()
            train_df = augmentation.augment_dataset(train_df, augmentation_factor=2, use_checkpoints=use_checkpoints)
            MemoryManager.check_memory_safe()
        
        # 2. Veri preprocessing
        print("\nðŸ§¹ VERÄ° PREPROCESSING")
        print("-" * 30)
        self.preprocessor = AddressPreprocessor()
        train_df['cleaned_address'] = train_df['address'].apply(self.preprocessor.clean_address)
        
        # Adresleri hazÄ±rla
        self.train_addresses = train_df['cleaned_address'].tolist()
        self.train_labels = train_df['label'].tolist()
        
        # Label encoder
        self.label_encoder = LabelEncoder()
        self.label_encoder.fit(self.train_labels)
        num_labels = len(self.label_encoder.classes_)
        
        # 3. Memory-efficient embedding'ler
        print("\nðŸ“ MEMORY-EFFICIENT EMBEDDING'LER")
        print("-" * 40)
        self.embedding_model = MemoryEfficientEmbeddingModel(device=self.device)
        self.train_embeddings = self.embedding_model.get_embeddings_memory_efficient(
            self.train_addresses, use_checkpoints=use_checkpoints
        )
        print(f"âœ… Embedding'ler hazÄ±rlandÄ±: {self.train_embeddings.shape}")
        
        # 4. UNIFIED MODEL EÄžÄ°TÄ°MÄ°
        print("\nðŸŽ¯ UNIFIED MODEL EÄžÄ°TÄ°MÄ°")
        print("-" * 40)
        self._train_unified_model_with_checkpoints(num_labels, use_checkpoints)
        
        # 5. Lightweight Ensemble (backup iÃ§in)
        print("\nðŸŒ² LIGHTWEIGHT ENSEMBLE (BACKUP)")
        print("-" * 40)
        self._train_lightweight_ensemble_with_checkpoints(use_checkpoints)
        
        # TAM MODEL CHECKPOINT KAYDET
        if use_checkpoints:
            print("\nðŸ’¾ TAM MODEL CHECKPOINT KAYDETME")
            print("-" * 40)
            model_checkpoint = f"full_model_{len(train_df)}_{hash(str(train_df['address'].iloc[:5].tolist()))}"
            model_data = {
                'train_embeddings': self.train_embeddings,
                'train_addresses': self.train_addresses,
                'train_labels': self.train_labels,
                'label_encoder': self.label_encoder,
                'preprocessor': self.preprocessor,
                'tfidf_vectorizer': self.tfidf_vectorizer,
                'rf_classifier': self.rf_classifier
            }
            
            # Unified model state'ini kaydet
            if self.unified_model:
                model_data['unified_model_state'] = self.unified_model.state_dict()
                print("âœ… Unified model state kaydedildi")
            
            metadata = {
                'data_size': len(train_df),
                'embedding_shape': self.train_embeddings.shape,
                'num_labels': num_labels
            }
            success = self.checkpoint_manager.save_checkpoint(model_checkpoint, model_data, metadata)
            if success:
                print(f"âœ… Tam model checkpoint kaydedildi: {model_checkpoint}")
            else:
                print("âŒ Checkpoint kaydetme hatasÄ±!")
        
        print("\nâœ… UNIFIED model eÄŸitimi tamamlandÄ±!")
        MemoryManager.check_memory_safe()
    
    def _train_unified_model(self, num_labels):
        """UNIFIED MODEL EÄžÄ°TÄ°MÄ° - TÃœM OPTÄ°MÄ°ZASYONLAR BÄ°RLEÅžTÄ°RÄ°LDÄ°"""
        print("ðŸŽ¯ Unified model eÄŸitiliyor...")
        
        # Unified model oluÅŸtur
        self.unified_model = UnifiedAddressModel(
            embedding_dim=384,
            num_labels=num_labels
        ).to(self.device)
        
        # Unified loss function
        criterion = UnifiedLoss(temperature=4.0, alpha=0.7)
        optimizer = torch.optim.AdamW(self.unified_model.parameters(), lr=5e-5, weight_decay=1e-5)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)
        
        # Labels
        encoded_labels = self.label_encoder.transform(self.train_labels)
        labels_tensor = torch.tensor(encoded_labels, dtype=torch.long).to(self.device)
        
        # EÄŸitim
        self.unified_model.train()
        batch_size = MemoryManager.optimize_batch_size(384, len(self.train_embeddings))
        embeddings_tensor = torch.tensor(self.train_embeddings, dtype=torch.float32).to(self.device)
        
        for epoch in range(5):  # Unified model iÃ§in daha fazla epoch
            total_loss = 0
            batch_count = 0
            
            for i in range(0, len(self.train_embeddings), batch_size):
                batch_embeddings = embeddings_tensor[i:i+batch_size]
                batch_labels = labels_tensor[i:i+batch_size]
                
                # Forward pass - tÃ¼m tasklar birlikte
                outputs = self.unified_model(batch_embeddings, task='all')
                
                # Teacher logits (sabit teacher weights kullan)
                teacher_logits = self.unified_model.get_teacher_logits(batch_embeddings)
                
                # Similarity pairs (basit positive/negative pairs)
                similarity_pairs = self._create_similarity_pairs(batch_embeddings, batch_labels)
                
                # Unified loss
                loss, loss_components = criterion(outputs, batch_labels, teacher_logits, similarity_pairs)
                
                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.unified_model.parameters(), max_norm=1.0)
                optimizer.step()
                
                total_loss += loss.item()
                batch_count += 1
            
            scheduler.step()
            avg_loss = total_loss / batch_count
            print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}")
            
            # Memory temizleme
            if epoch % 2 == 0:
                MemoryManager.check_memory_safe()
        
        print("âœ… Unified model eÄŸitimi tamamlandÄ±")
    
    def _train_unified_model_with_checkpoints(self, num_labels, use_checkpoints=True):
        """UNIFIED MODEL EÄžÄ°TÄ°MÄ° - CHECKPOINT SÄ°STEMÄ° Ä°LE"""
        print("ðŸŽ¯ Unified model eÄŸitiliyor...")
        
        # Model checkpoint kontrolÃ¼
        if use_checkpoints:
            model_checkpoint = f"unified_model_{num_labels}_{hash(str(self.train_addresses[:5]))}"
            
            if self.checkpoint_manager.checkpoint_exists(model_checkpoint):
                print("ðŸ“‚ Model checkpoint bulundu, yÃ¼kleniyor...")
                model_data, metadata = self.checkpoint_manager.load_checkpoint(model_checkpoint)
                
                if model_data is not None:
                    self.unified_model = model_data['model']
                    print("âœ… Unified model checkpoint'ten yÃ¼klendi")
                    return
        
        # Yeni model eÄŸitimi
        self.unified_model = UnifiedAddressModel(
            embedding_dim=384,
            num_labels=num_labels
        ).to(self.device)
        
        # Unified loss function
        criterion = UnifiedLoss(temperature=4.0, alpha=0.7)
        optimizer = torch.optim.AdamW(self.unified_model.parameters(), lr=5e-5, weight_decay=1e-5)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)
        
        # Labels
        encoded_labels = self.label_encoder.transform(self.train_labels)
        labels_tensor = torch.tensor(encoded_labels, dtype=torch.long).to(self.device)
        
        # EÄŸitim
        self.unified_model.train()
        batch_size = MemoryManager.optimize_batch_size(384, len(self.train_embeddings))
        embeddings_tensor = torch.tensor(self.train_embeddings, dtype=torch.float32).to(self.device)
        
        for epoch in range(5):  # Unified model iÃ§in daha fazla epoch
            total_loss = 0
            batch_count = 0
            
            for i in range(0, len(self.train_embeddings), batch_size):
                batch_embeddings = embeddings_tensor[i:i+batch_size]
                batch_labels = labels_tensor[i:i+batch_size]
                
                # Forward pass - tÃ¼m tasklar birlikte
                outputs = self.unified_model(batch_embeddings, task='all')
                
                # Teacher logits (sabit teacher weights kullan)
                teacher_logits = self.unified_model.get_teacher_logits(batch_embeddings)
                
                # Similarity pairs (basit positive/negative pairs)
                similarity_pairs = self._create_similarity_pairs(batch_embeddings, batch_labels)
                
                # Unified loss
                loss, loss_components = criterion(outputs, batch_labels, teacher_logits, similarity_pairs)
                
                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.unified_model.parameters(), max_norm=1.0)
                optimizer.step()
                
                total_loss += loss.item()
                batch_count += 1
            
            scheduler.step()
            avg_loss = total_loss / batch_count
            print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}")
            
            # Epoch checkpoint kaydet
            if use_checkpoints and epoch % 2 == 0:
                epoch_checkpoint = f"unified_model_epoch_{epoch}_{num_labels}_{hash(str(self.train_addresses[:5]))}"
                model_data = {
                    'model': self.unified_model,
                    'optimizer_state': optimizer.state_dict(),
                    'scheduler_state': scheduler.state_dict(),
                    'epoch': epoch,
                    'loss': avg_loss
                }
                metadata = {
                    'epoch': epoch,
                    'loss': avg_loss,
                    'num_labels': num_labels
                }
                self.checkpoint_manager.save_checkpoint(epoch_checkpoint, model_data, metadata)
            
            # Memory temizleme
            if epoch % 2 == 0:
                MemoryManager.check_memory_safe()
        
        # Final model checkpoint kaydet
        if use_checkpoints:
            model_checkpoint = f"unified_model_{num_labels}_{hash(str(self.train_addresses[:5]))}"
            model_data = {
                'model': self.unified_model,
                'optimizer_state': optimizer.state_dict(),
                'scheduler_state': scheduler.state_dict()
            }
            metadata = {
                'num_labels': num_labels,
                'final_loss': avg_loss
            }
            self.checkpoint_manager.save_checkpoint(model_checkpoint, model_data, metadata)
        
        print("âœ… Unified model eÄŸitimi tamamlandÄ±")
    
    def _train_lightweight_ensemble_with_checkpoints(self, use_checkpoints=True):
        """Hafif ensemble eÄŸitimi - CHECKPOINT SÄ°STEMÄ° Ä°LE"""
        print("ðŸŒ² Hafif ensemble eÄŸitiliyor...")
        
        # Ensemble checkpoint kontrolÃ¼
        if use_checkpoints:
            ensemble_checkpoint = f"ensemble_{len(self.train_addresses)}_{hash(str(self.train_addresses[:5]))}"
            
            if self.checkpoint_manager.checkpoint_exists(ensemble_checkpoint):
                print("ðŸ“‚ Ensemble checkpoint bulundu, yÃ¼kleniyor...")
                ensemble_data, metadata = self.checkpoint_manager.load_checkpoint(ensemble_checkpoint)
                
                if ensemble_data is not None:
                    self.tfidf_vectorizer = ensemble_data['tfidf_vectorizer']
                    self.rf_classifier = ensemble_data['rf_classifier']
                    print("âœ… Ensemble checkpoint'ten yÃ¼klendi")
                    return
        
        # Yeni ensemble eÄŸitimi
        # TF-IDF (daha az feature)
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=2000,  # Daha az feature
            ngram_range=(1, 2),  # Daha kÄ±sa n-gram
            min_df=3,
            max_df=0.9
        )
        tfidf_features = self.tfidf_vectorizer.fit_transform(self.train_addresses)
        print(f"âœ… TF-IDF features: {tfidf_features.shape}")
        
        # Random Forest (daha az tree)
        encoded_labels = self.label_encoder.transform(self.train_labels)
        self.rf_classifier = RandomForestClassifier(
            n_estimators=50,  # Daha az tree
            max_depth=15,     # Daha az depth
            random_state=42,
            n_jobs=-1
        )
        self.rf_classifier.fit(tfidf_features, encoded_labels)
        print("âœ… Random Forest eÄŸitildi")
        
        # Ensemble checkpoint kaydet
        if use_checkpoints:
            ensemble_checkpoint = f"ensemble_{len(self.train_addresses)}_{hash(str(self.train_addresses[:5]))}"
            ensemble_data = {
                'tfidf_vectorizer': self.tfidf_vectorizer,
                'rf_classifier': self.rf_classifier
            }
            metadata = {
                'data_size': len(self.train_addresses),
                'tfidf_shape': tfidf_features.shape
            }
            self.checkpoint_manager.save_checkpoint(ensemble_checkpoint, ensemble_data, metadata)
    
    def _create_similarity_pairs(self, embeddings, labels):
        """Similarity pairs oluÅŸtur"""
        batch_size = embeddings.size(0)
        pos_pairs = []
        neg_pairs = []
        
        # Positive pairs (aynÄ± label)
        for i in range(batch_size):
            for j in range(i+1, min(i+3, batch_size)):  # Her sample iÃ§in max 2 positive pair
                if labels[i] == labels[j]:
                    pos_pairs.append((embeddings[i], embeddings[j]))
        
        # Negative pairs (farklÄ± label)
        for i in range(min(len(pos_pairs), batch_size)):
            idx1 = torch.randint(0, batch_size, (1,)).item()
            idx2 = torch.randint(0, batch_size, (1,)).item()
            if labels[idx1] != labels[idx2]:
                neg_pairs.append((embeddings[idx1], embeddings[idx2]))
        
        # Tensor'a Ã§evir
        if pos_pairs:
            pos_emb = torch.stack([pair[0] for pair in pos_pairs])
            pos_emb2 = torch.stack([pair[1] for pair in pos_pairs])
        else:
            pos_emb = torch.empty(0, embeddings.size(1)).to(embeddings.device)
            pos_emb2 = torch.empty(0, embeddings.size(1)).to(embeddings.device)
        
        if neg_pairs:
            neg_emb = torch.stack([pair[0] for pair in neg_pairs])
            neg_emb2 = torch.stack([pair[1] for pair in neg_pairs])
        else:
            neg_emb = torch.empty(0, embeddings.size(1)).to(embeddings.device)
            neg_emb2 = torch.empty(0, embeddings.size(1)).to(embeddings.device)
        
        return (pos_emb, pos_emb2), (neg_emb, neg_emb2)
    
    def _train_lightweight_ensemble(self):
        """Hafif ensemble eÄŸitimi"""
        print("ðŸŒ² Hafif ensemble eÄŸitiliyor...")
        
        # TF-IDF (daha az feature)
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=2000,  # Daha az feature
            ngram_range=(1, 2),  # Daha kÄ±sa n-gram
            min_df=3,
            max_df=0.9
        )
        tfidf_features = self.tfidf_vectorizer.fit_transform(self.train_addresses)
        print(f"âœ… TF-IDF features: {tfidf_features.shape}")
        
        # Random Forest (daha az tree)
        encoded_labels = self.label_encoder.transform(self.train_labels)
        self.rf_classifier = RandomForestClassifier(
            n_estimators=50,  # Daha az tree
            max_depth=15,     # Daha az depth
            random_state=42,
            n_jobs=-1
        )
        self.rf_classifier.fit(tfidf_features, encoded_labels)
        print("âœ… Random Forest eÄŸitildi")
    
    def predict_label(self, query_address, method='unified'):
        """UNIFIED MODEL TAHMÄ°NÄ° - TEK YÃ–NTEM"""
        if method == 'unified' and self.unified_model:
            return self._unified_predict(query_address)
        elif method == 'ensemble':
            return self._ensemble_predict(query_address)
        elif method == 'fuzzy':
            # Fuzzy string matching
            cleaned_query = self.preprocessor.clean_address(query_address)
            best_match = process.extractOne(cleaned_query, self.train_addresses, scorer=fuzz.token_sort_ratio)
            if best_match and best_match[1] > 80:  # %80 Ã¼zeri benzerlik
                idx = self.train_addresses.index(best_match[0])
                return self.train_labels[idx]
            return None
        elif method == 'random_forest' and self.rf_classifier:
            # Random Forest tahmini
            cleaned_query = self.preprocessor.clean_address(query_address)
            tfidf_features = self.tfidf_vectorizer.transform([cleaned_query])
            predicted_encoded = self.rf_classifier.predict(tfidf_features)[0]
            return self.label_encoder.inverse_transform([predicted_encoded])[0]
        else:
            # Default: similarity
            return self._similarity_predict(query_address)
    
    def _hierarchical_predict(self, query_address):
        """Hierarchical tahmin"""
        # 1. BÃ¶lge tahmini
        region_features = self.hierarchical_classifier.extract_region_features(query_address)
        # Basit region belirleme
        if any(region_features.values()):
            region = 'urban'
        else:
            region = 'rural'
        
        # 2. O bÃ¶lgedeki adresler arasÄ±ndan tahmin
        region_labels = self.hierarchical_classifier.region_mapping.get(region, [])
        if not region_labels:
            return self._similarity_predict(query_address)
        
        # Sadece o bÃ¶lgedeki embedding'leri kullan
        region_indices = [i for i, label in enumerate(self.train_labels) if label in region_labels]
        if not region_indices:
            return self._similarity_predict(query_address)
        
        region_embeddings = self.train_embeddings[region_indices]
        region_train_labels = [self.train_labels[i] for i in region_indices]
        
        # Similarity hesapla
        query_embedding = self.embedding_model.model.encode([query_address])
        similarities = cosine_similarity(query_embedding, region_embeddings)[0]
        best_idx = np.argmax(similarities)
        
        return region_train_labels[best_idx]
    
    def _distilled_predict(self, query_address):
        """Distilled model tahmini"""
        query_embedding = self.embedding_model.model.encode([query_address])
        query_tensor = torch.tensor(query_embedding, dtype=torch.float32).to(self.device)
        
        self.distilled_model.eval()
        with torch.no_grad():
            logits = self.distilled_model(query_tensor)
            predicted_idx = torch.argmax(logits, dim=1).item()
        
        return self.label_encoder.inverse_transform([predicted_idx])[0]
    
    def _multi_task_predict(self, query_address):
        """Multi-task model tahmini"""
        query_embedding = self.embedding_model.model.encode([query_address])
        query_tensor = torch.tensor(query_embedding, dtype=torch.float32).to(self.device)
        
        self.multi_task_model.eval()
        with torch.no_grad():
            main_logits, _, _ = self.multi_task_model(query_tensor)
            predicted_idx = torch.argmax(main_logits, dim=1).item()
        
        return self.label_encoder.inverse_transform([predicted_idx])[0]
    
    def _ensemble_predict(self, query_address):
        """Ensemble tahmin"""
        predictions = {}
        weights = {}
        
        # Similarity prediction
        try:
            similar_result = self._similarity_predict(query_address)
            predictions['similarity'] = similar_result
            weights['similarity'] = 1.0
        except:
            pass
        
        # Distilled prediction
        if self.distilled_model:
            try:
                distilled_result = self._distilled_predict(query_address)
                predictions['distilled'] = distilled_result
                weights['distilled'] = 0.8
            except:
                pass
        
        # Multi-task prediction
        if self.multi_task_model:
            try:
                multitask_result = self._multi_task_predict(query_address)
                predictions['multi_task'] = multitask_result
                weights['multi_task'] = 0.9
            except:
                pass
        
        # Hierarchical prediction
        if self.hierarchical_classifier:
            try:
                hierarchical_result = self._hierarchical_predict(query_address)
                predictions['hierarchical'] = hierarchical_result
                weights['hierarchical'] = 0.7
            except:
                pass
        
        # Weighted voting
        if predictions:
            vote_counts = {}
            for method, prediction in predictions.items():
                weight = weights.get(method, 1.0)
                if prediction not in vote_counts:
                    vote_counts[prediction] = 0
                vote_counts[prediction] += weight
            
            # En yÃ¼ksek oy alan tahmini dÃ¶ndÃ¼r
            best_prediction = max(vote_counts.items(), key=lambda x: x[1])[0]
            return best_prediction
        
        # Fallback
        return self._similarity_predict(query_address)
    
    def _similarity_predict(self, query_address):
        """Basit similarity tahmini"""
        query_embedding = self.embedding_model.model.encode([query_address])
        similarities = cosine_similarity(query_embedding, self.train_embeddings)[0]
        best_idx = np.argmax(similarities)
        return self.train_labels[best_idx]
    
    def find_similar_addresses(self, query_address, top_k=5, method='ensemble'):
        """Benzer adresleri bul"""
        if self.embedding_model is None:
            raise ValueError("Model henÃ¼z eÄŸitilmemiÅŸ!")
        
        # Adres temizleme
        cleaned_query = self.preprocessor.clean_address(query_address)
        
        # Query embedding
        query_embedding = self.embedding_model.model.encode([cleaned_query])
        
        # Similarity hesapla
        similarities = cosine_similarity(query_embedding, self.train_embeddings)[0]
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append({
                'label': self.train_labels[idx],
                'similarity': similarities[idx],
                'address': self.train_addresses[idx],
                'index': idx
            })
        
        return results
    
    def _unified_predict(self, query_address):
        """UNIFIED MODEL TAHMÄ°NÄ° - TÃœM OPTÄ°MÄ°ZASYONLAR BÄ°RLÄ°KTE"""
        # Query embedding
        query_embedding = self.embedding_model.model.encode([query_address])
        query_tensor = torch.tensor(query_embedding, dtype=torch.float32).to(self.device)
        
        # Unified model tahmini
        self.unified_model.eval()
        with torch.no_grad():
            # TÃ¼m tasklar birlikte Ã§alÄ±ÅŸÄ±r
            outputs = self.unified_model(query_tensor, task='all')
            
            # Ana sÄ±nÄ±flandÄ±rma tahmini
            main_logits = outputs['main']
            predicted_idx = torch.argmax(main_logits, dim=1).item()
            
            # Confidence score
            probabilities = F.softmax(main_logits, dim=1)
            confidence = probabilities[0][predicted_idx].item()
            
            # EÄŸer confidence dÃ¼ÅŸÃ¼kse, similarity ile backup
            if confidence < 0.5:
                print(f"âš ï¸ DÃ¼ÅŸÃ¼k confidence ({confidence:.3f}), similarity backup kullanÄ±lÄ±yor...")
                return self._similarity_predict(query_address)
            
            predicted_label = self.label_encoder.inverse_transform([predicted_idx])[0]
            
            # Debug bilgileri
            print(f"ðŸŽ¯ Unified Model Tahmini:")
            print(f"  - Predicted Label: {predicted_label}")
            print(f"  - Confidence: {confidence:.3f}")
            print(f"  - Region: {torch.argmax(outputs['region'], dim=1).item()}")
            print(f"  - Type: {torch.argmax(outputs['type'], dim=1).item()}")
            
            return predicted_label
    
    def save_model(self, filepath='models/bert_address_model.pkl'):
        """UNIFIED MODEL KAYDETME"""
        print(f"ðŸ’¾ Unified model kaydediliyor: {filepath}")
        
        model_data = {
            'train_embeddings': self.train_embeddings,
            'train_labels': self.train_labels,
            'train_addresses': self.train_addresses,
            'label_encoder': self.label_encoder,
            'tfidf_vectorizer': self.tfidf_vectorizer,
            'rf_classifier': self.rf_classifier,
            'preprocessor': self.preprocessor,
            'device': str(self.device)
        }
        
        # Unified model'i kaydet
        if self.unified_model:
            unified_path = filepath.replace('.pkl', '_unified.pth')
            torch.save(self.unified_model.state_dict(), unified_path)
            model_data['unified_model_path'] = unified_path
        
        joblib.dump(model_data, filepath)
        
        print(f"âœ… Unified model baÅŸarÄ±yla kaydedildi: {filepath}")
    
    @classmethod
    def load_model(cls, filepath='models/bert_address_model.pkl'):
        """UNIFIED MODEL YÃœKLEME"""
        print(f"ðŸ“‚ Unified model yÃ¼kleniyor: {filepath}")
        
        model_data = joblib.load(filepath)
        
        # Model nesnesini oluÅŸtur
        device = torch.device(model_data.get('device', 'cpu'))
        model = cls(device=device)
        
        model.embedding_model = MemoryEfficientEmbeddingModel(device=device)
        model.train_embeddings = model_data['train_embeddings']
        model.train_labels = model_data['train_labels']
        model.train_addresses = model_data['train_addresses']
        model.label_encoder = model_data.get('label_encoder')
        model.tfidf_vectorizer = model_data.get('tfidf_vectorizer')
        model.rf_classifier = model_data.get('rf_classifier')
        model.preprocessor = model_data.get('preprocessor', AddressPreprocessor())
        
        # Unified model'i yÃ¼kle
        if 'unified_model_path' in model_data:
            try:
                num_labels = len(model.label_encoder.classes_) if model.label_encoder else 10390
                model.unified_model = UnifiedAddressModel(
                    embedding_dim=384,
                    num_labels=num_labels
                ).to(device)
                model.unified_model.load_state_dict(torch.load(model_data['unified_model_path'], map_location=device))
                model.unified_model.eval()
                print("âœ… Unified model yÃ¼klendi")
            except Exception as e:
                print(f"âš ï¸ Unified model yÃ¼kleme hatasÄ±: {e}")
        
        print(f"âœ… Unified model baÅŸarÄ±yla yÃ¼klendi!")
        return model

# ============================================================================
# ðŸš€ ORIGINAL CLASSES (Mevcut kod korunuyor)
# ============================================================================

class AddressPreprocessor:
    """GeliÅŸmiÅŸ adres preprocessing sÄ±nÄ±fÄ±"""
    
    def __init__(self):
        # TÃ¼rkÃ§e adres kalÄ±plarÄ±
        self.turkish_patterns = {
            'mahalle': r'\b(mahalle|mah|mh\.)\b',
            'sokak': r'\b(sokak|sok|sk\.)\b',
            'cadde': r'\b(cadde|cad|c\.)\b',
            'bulvar': r'\b(bulvar|bulv|blv\.)\b',
            'apartman': r'\b(apartman|apt\.)\b',
            'kat': r'\b(kat|k\.)\b',
            'numara': r'\b(numara|no|nr\.)\b'
        }
        
        # Temizleme kalÄ±plarÄ±
        self.cleanup_patterns = [
            (r'\s+', ' '),  # Fazla boÅŸluklarÄ± temizle
            (r'[^\w\sÃ§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄžIÄ°Ã–ÅžÃœ\-\.]', ''),  # Ã–zel karakterleri temizle
            (r'\b\d+\s*(kat|k\.)', ''),  # Kat bilgilerini temizle
            (r'\b\d+\s*(daire|d\.)', ''),  # Daire bilgilerini temizle
        ]
    
    def clean_address(self, address):
        """Adres temizleme"""
        if pd.isna(address):
            return ""
        
        address = str(address).lower().strip()
        
        # Temizleme iÅŸlemleri
        for pattern, replacement in self.cleanup_patterns:
            address = re.sub(pattern, replacement, address)
        
        # TÃ¼rkÃ§e karakterleri normalize et
        address = self._normalize_turkish_chars(address)
        
        # Fazla boÅŸluklarÄ± temizle
        address = ' '.join(address.split())
        
        return address
    
    def _normalize_turkish_chars(self, text):
        """TÃ¼rkÃ§e karakterleri normalize et"""
        replacements = {
            'Ã§': 'c', 'ÄŸ': 'g', 'Ä±': 'i', 'Ã¶': 'o', 'ÅŸ': 's', 'Ã¼': 'u',
            'Ã‡': 'C', 'Äž': 'G', 'I': 'I', 'Ä°': 'I', 'Ã–': 'O', 'Åž': 'S', 'Ãœ': 'U'
        }
        
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        return text
    
    def extract_features(self, address):
        """Adres Ã¶zelliklerini Ã§Ä±kar"""
        features = {}
        
        # Uzunluk Ã¶zellikleri
        features['length'] = len(address)
        features['word_count'] = len(address.split())
        
        # TÃ¼rkÃ§e adres kalÄ±plarÄ±
        for pattern_name, pattern in self.turkish_patterns.items():
            features[f'has_{pattern_name}'] = 1 if re.search(pattern, address, re.IGNORECASE) else 0
        
        # SayÄ±sal Ã¶zellikler
        features['has_numbers'] = 1 if re.search(r'\d', address) else 0
        features['number_count'] = len(re.findall(r'\d+', address))
        
        return features

class AddressDataset(Dataset):
    """GeliÅŸtirilmiÅŸ adres veri seti"""
    
    def __init__(self, addresses, labels, tokenizer, max_length=512, preprocessor=None):
        self.addresses = addresses
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.preprocessor = preprocessor or AddressPreprocessor()
        
        # Adresleri temizle
        self.cleaned_addresses = [self.preprocessor.clean_address(addr) for addr in addresses]
        
    def __len__(self):
        return len(self.addresses)
    
    def __getitem__(self, idx):
        address = self.cleaned_addresses[idx]
        label = self.labels[idx]
        
        # Tokenize
        encoding = self.tokenizer(
            address,
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }

class AdvancedTurkishAddressBERT(nn.Module):
    """GeliÅŸtirilmiÅŸ TÃ¼rkÃ§e BERT tabanlÄ± adres sÄ±nÄ±flandÄ±rma modeli"""
    
    def __init__(self, model_name="dbmdz/bert-base-turkish-cased", num_labels=10390, dropout_rate=0.3):
        super().__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout_rate)
        
        # GeliÅŸtirilmiÅŸ classifier
        self.classifier = nn.Sequential(
            nn.Linear(768, 512),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, num_labels)
        )
        
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits

class AddressEmbeddingModel:
    """GeliÅŸtirilmiÅŸ adres embedding modeli"""
    
    def __init__(self, model_name="sentence-transformers/all-MiniLM-L6-v2", device=None):
        self.model = SentenceTransformer(model_name)
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
    def get_embeddings(self, addresses, batch_size=64):
        """Adresler iÃ§in embedding'leri al - GPU optimizasyonu ile"""
        print("ðŸ“ Adres embedding'leri hesaplanÄ±yor...")
        
        # Batch halinde iÅŸle
        all_embeddings = []
        for i in tqdm(range(0, len(addresses), batch_size), desc="Embedding calculation"):
            batch = addresses[i:i+batch_size]
            embeddings = self.model.encode(batch, show_progress_bar=False, batch_size=batch_size)
            all_embeddings.append(embeddings)
        
        return np.vstack(all_embeddings)
    
    def find_similar_advanced(self, query_address, train_embeddings, train_labels, top_k=5, method='cosine'):
        """GeliÅŸmiÅŸ benzerlik hesaplama"""
        query_embedding = self.model.encode([query_address])
        
        if method == 'cosine':
            similarities = cosine_similarity(query_embedding, train_embeddings)[0]
        elif method == 'euclidean':
            distances = euclidean_distances(query_embedding, train_embeddings)[0]
            similarities = 1 / (1 + distances)  # Distance'Ä± similarity'ye Ã§evir
        else:
            raise ValueError(f"Bilinmeyen method: {method}")
        
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append({
                'label': train_labels[idx],
                'similarity': similarities[idx],
                'index': idx
            })
        
        return results

class SiameseAddressMatcher(nn.Module):
    """GeliÅŸtirilmiÅŸ Siamese network adres eÅŸleÅŸtirme"""
    
    def __init__(self, embedding_dim=384, hidden_dims=[256, 128, 64]):
        super().__init__()
        
        layers = []
        input_dim = embedding_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(input_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.3)
            ])
            input_dim = hidden_dim
        
        self.embedding_net = nn.Sequential(*layers)
        
    def forward_one(self, x):
        return self.embedding_net(x)
    
    def forward(self, x1, x2):
        out1 = self.forward_one(x1)
        out2 = self.forward_one(x2)
        return out1, out2

class ContrastiveLoss(nn.Module):
    """GeliÅŸtirilmiÅŸ Contrastive loss"""
    
    def __init__(self, margin=1.0, temperature=0.1):
        super().__init__()
        self.margin = margin
        self.temperature = temperature
        
    def forward(self, x1, x2, y):
        # Cosine similarity
        cos_sim = F.cosine_similarity(x1, x2, dim=1)
        
        # Temperature scaling
        cos_sim = cos_sim / self.temperature
        
        # Contrastive loss
        loss = y * torch.pow(1 - cos_sim, 2) + (1 - y) * torch.pow(torch.clamp(cos_sim - self.margin, min=0.0), 2)
        return loss.mean()

class EnsembleAddressMatcher:
    """Ensemble adres eÅŸleÅŸtirme modeli"""
    
    def __init__(self):
        self.models = {}
        self.weights = {}
        
    def add_model(self, name, model, weight=1.0):
        """Modele model ekle"""
        self.models[name] = model
        self.weights[name] = weight
    
    def predict(self, query_address, top_k=5):
        """Ensemble tahmin"""
        all_predictions = {}
        
        for name, model in self.models.items():
            try:
                if name == 'sentence_transformer':
                    # Sentence transformer iÃ§in direkt embedding kullan
                    predictions = model._get_sentence_transformer_predictions(query_address, top_k=top_k*2)
                elif name == 'random_forest':
                    # Random forest iÃ§in direkt tahmin
                    predictions = model._get_random_forest_predictions(query_address, top_k=top_k*2)
                else:
                    predictions = model.find_similar_addresses(query_address, top_k=top_k*2)
                
                weight = self.weights[name]
                
                for pred in predictions:
                    label = pred['label']
                    similarity = pred.get('similarity', pred.get('score', 0)) * weight
                    
                    if label not in all_predictions:
                        all_predictions[label] = {'total_score': 0, 'count': 0}
                    
                    all_predictions[label]['total_score'] += similarity
                    all_predictions[label]['count'] += 1
                    
            except Exception as e:
                print(f"âš ï¸ Model {name} hatasÄ±: {e}")
        
        # En iyi tahminleri seÃ§
        sorted_predictions = sorted(
            all_predictions.items(),
            key=lambda x: x[1]['total_score'] / x[1]['count'],
            reverse=True
        )
        
        results = []
        for label, info in sorted_predictions[:top_k]:
            results.append({
                'label': label,
                'score': info['total_score'] / info['count'],
                'votes': info['count']
            })
        
        return results

class AdvancedBERTAddressMatcher:
    """GeliÅŸtirilmiÅŸ BERT tabanlÄ± adres eÅŸleÅŸtirme ana sÄ±nÄ±fÄ± (Optimized)"""
    
    def __init__(self, device=None):
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.embedding_model = None
        self.siamese_model = None
        self.ensemble_model = None
        self.preprocessor = AddressPreprocessor()
        self.train_embeddings = None
        self.train_labels = None
        self.train_addresses = None
        self.label_encoder = None
        self.tfidf_vectorizer = None
        self.rf_classifier = None
        
        # Yeni optimize edilmiÅŸ bileÅŸenler
        self.distilled_model = None
        self.hierarchical_classifier = None
        self.multi_task_model = None
        
        print(f"ðŸš€ Optimized BERT Address Matcher")
        print(f"ðŸ“± Cihaz: {self.device}")
        if torch.cuda.is_available():
            print(f"ðŸŽ® GPU: {torch.cuda.get_device_name()}")
            print(f"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        
        # Memory kontrolÃ¼
        MemoryManager.check_memory_safe()
    
    def _get_sentence_transformer_predictions(self, query_address, top_k=5):
        """Sentence transformer iÃ§in direkt tahmin"""
        if self.embedding_model is None:
            return []
        
        cleaned_query = self.preprocessor.clean_address(query_address)
        results = self.embedding_model.find_similar_advanced(
            cleaned_query, 
            self.train_embeddings, 
            self.train_labels, 
            top_k,
            method='cosine'
        )
        
        # Adres bilgilerini ekle
        for result in results:
            label = result['label']
            matching_indices = [i for i, l in enumerate(self.train_labels) if l == label]
            if matching_indices:
                result['address'] = self.train_addresses[matching_indices[0]]
        
        return results
    
    def _get_random_forest_predictions(self, query_address, top_k=5):
        """Random forest iÃ§in direkt tahmin"""
        if self.rf_classifier is None:
            return []
        
        try:
            cleaned_query = self.preprocessor.clean_address(query_address)
            tfidf_features = self.tfidf_vectorizer.transform([cleaned_query])
            
            # En olasÄ± 5 sÄ±nÄ±fÄ± al
            probabilities = self.rf_classifier.predict_proba(tfidf_features)[0]
            top_indices = np.argsort(probabilities)[::-1][:top_k]
            
            results = []
            for idx in top_indices:
                label = self.label_encoder.inverse_transform([idx])[0]
                probability = probabilities[idx]
                
                # Bu label'a ait bir adres bul
                matching_indices = [i for i, l in enumerate(self.train_labels) if l == label]
                if matching_indices:
                    address = self.train_addresses[matching_indices[0]]
                else:
                    address = "Bilinmeyen adres"
                
                results.append({
                    'label': label,
                    'similarity': probability,
                    'address': address
                })
            
            return results
        except Exception as e:
            print(f"âš ï¸ Random Forest tahmin hatasÄ±: {e}")
            return []
    
    def fit(self, train_df, use_ensemble=True, use_siamese=True, use_distillation=True, use_hierarchical=True, use_augmentation=True, use_multi_task=True):
        """GeliÅŸtirilmiÅŸ model eÄŸitimi - TÃœM OPTÄ°MÄ°ZASYONLAR"""
        print("ðŸš€ OPTÄ°MÄ°ZE EDÄ°LMÄ°Åž BERT ADRES EÅžLEÅžTÄ°RME MODELÄ° EÄžÄ°TÄ°MÄ°")
        print("=" * 80)
        print(f"ðŸ“Š EÄŸitim verisi boyutu: {len(train_df)}")
        
        # Memory kontrolÃ¼
        MemoryManager.check_memory_safe()
        
        # 1. Data Augmentation
        if use_augmentation:
            print("\nðŸ”„ ADVANCED DATA AUGMENTATION")
            print("-" * 40)
            augmentation = AdvancedDataAugmentation()
            train_df = augmentation.augment_dataset(train_df, augmentation_factor=2, use_checkpoints=True)
            MemoryManager.check_memory_safe()
        
        # 2. Hierarchical Classification
        if use_hierarchical:
            print("\nðŸ—ï¸ HIERARCHICAL CLASSIFICATION")
            print("-" * 40)
            self.hierarchical_classifier = HierarchicalAddressClassifier()
            train_df = self.hierarchical_classifier.create_hierarchical_structure(train_df)
            MemoryManager.check_memory_safe()
        
        # 3. Veri preprocessing
        print("\nðŸ§¹ VERÄ° PREPROCESSING")
        print("-" * 30)
        train_df['cleaned_address'] = train_df['address'].apply(self.preprocessor.clean_address)
        
        # Adresleri hazÄ±rla
        self.train_addresses = train_df['cleaned_address'].tolist()
        self.train_labels = train_df['label'].tolist()
        
        # Label encoder
        self.label_encoder = LabelEncoder()
        self.label_encoder.fit(self.train_labels)
        
        # 4. Memory-efficient embedding'ler
        print("\nðŸ“ MEMORY-EFFICIENT EMBEDDING'LER")
        print("-" * 40)
        self.embedding_model = MemoryEfficientEmbeddingModel(device=self.device)
        self.train_embeddings = self.embedding_model.get_embeddings_memory_efficient(
            self.train_addresses, use_checkpoints=True
        )
        print(f"âœ… Embedding'ler hazÄ±rlandÄ±: {self.train_embeddings.shape}")
        
        # 5. Multi-Task Learning
        if use_multi_task:
            print("\nðŸŽ¯ MULTI-TASK LEARNING")
            print("-" * 40)
            self._train_multi_task_model(len(self.label_encoder.classes_))
        
        # 6. Distillation
        if use_distillation:
            print("\nðŸŽ“ KNOWLEDGE DISTILLATION")
            print("-" * 40)
            self._train_distilled_model(len(self.label_encoder.classes_))
        
        # 7. TF-IDF Vectorizer (hafif versiyon)
        if use_ensemble:
            print("\nðŸ“Š LIGHTWEIGHT TF-IDF VECTORIZER")
            print("-" * 40)
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=2000,  # Daha az feature
                ngram_range=(1, 2),  # Daha kÄ±sa n-gram
                min_df=3,
                max_df=0.9
            )
            tfidf_features = self.tfidf_vectorizer.fit_transform(self.train_addresses)
            print(f"âœ… TF-IDF features: {tfidf_features.shape}")
        
        # 8. Random Forest Classifier (hafif versiyon)
        if use_ensemble:
            print("\nðŸŒ² LIGHTWEIGHT RANDOM FOREST CLASSIFIER")
            print("-" * 40)
            encoded_labels = self.label_encoder.transform(self.train_labels)
            self.rf_classifier = RandomForestClassifier(
                n_estimators=50,  # Daha az tree
                max_depth=15,     # Daha az depth
                random_state=42,
                n_jobs=-1
            )
            self.rf_classifier.fit(tfidf_features, encoded_labels)
            print("âœ… Random Forest eÄŸitildi")
        
        # 9. Siamese network eÄŸitimi
        if use_siamese:
            print("\nðŸ”§ SÄ°AMESE NETWORK EÄžÄ°TÄ°MÄ°")
            print("-" * 30)
            self._train_siamese_network()
        
        # 10. Ensemble model oluÅŸtur
        if use_ensemble:
            print("\nðŸŽ¯ ADVANCED ENSEMBLE MODEL OLUÅžTURMA")
            print("-" * 40)
            self.ensemble_model = EnsembleAddressMatcher()
            self.ensemble_model.add_model('sentence_transformer', self, weight=1.0)
            if self.rf_classifier:
                self.ensemble_model.add_model('random_forest', self, weight=0.7)
            if self.distilled_model:
                self.ensemble_model.add_model('distilled', self, weight=0.8)
            if self.multi_task_model:
                self.ensemble_model.add_model('multi_task', self, weight=0.9)
            print("âœ… Advanced ensemble model hazÄ±rlandÄ±")
        
        print("\nâœ… Optimize edilmiÅŸ model eÄŸitimi tamamlandÄ±!")
        
        # Memory temizleme
        MemoryManager.check_memory_safe()
    
    def _train_siamese_network(self):
        """GeliÅŸtirilmiÅŸ Siamese network eÄŸitimi"""
        # Veri Ã§iftleri oluÅŸtur
        positive_pairs, negative_pairs = self._create_training_pairs()
        
        if len(positive_pairs) == 0 or len(negative_pairs) == 0:
            print("âš ï¸ Yeterli veri Ã§ifti bulunamadÄ±, Siamese network atlanÄ±yor")
            return
        
        # Model oluÅŸtur
        self.siamese_model = SiameseAddressMatcher().to(self.device)
        criterion = ContrastiveLoss()
        optimizer = torch.optim.AdamW(self.siamese_model.parameters(), lr=1e-4, weight_decay=1e-5)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)
        
        # EÄŸitim
        self.siamese_model.train()
        best_loss = float('inf')
        patience = 3
        patience_counter = 0
        
        for epoch in range(10):
            total_loss = 0
            batch_count = 0
            
            # Positive pairs
            for i in range(0, len(positive_pairs), 64):
                batch_pairs = positive_pairs[i:i+64]
                if len(batch_pairs) < 2:
                    continue
                
                # Embedding'leri al
                addr1_emb = torch.tensor([self.train_embeddings[pair[0]] for pair in batch_pairs], dtype=torch.float32).to(self.device)
                addr2_emb = torch.tensor([self.train_embeddings[pair[1]] for pair in batch_pairs], dtype=torch.float32).to(self.device)
                labels = torch.ones(len(batch_pairs), dtype=torch.float32).to(self.device)
                
                # Forward pass
                out1, out2 = self.siamese_model(addr1_emb, addr2_emb)
                loss = criterion(out1, out2, labels)
                
                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.siamese_model.parameters(), max_norm=1.0)
                optimizer.step()
                
                total_loss += loss.item()
                batch_count += 1
            
            # Negative pairs
            for i in range(0, len(negative_pairs), 64):
                batch_pairs = negative_pairs[i:i+64]
                if len(batch_pairs) < 2:
                    continue
                
                # Embedding'leri al
                addr1_emb = torch.tensor([self.train_embeddings[pair[0]] for pair in batch_pairs], dtype=torch.float32).to(self.device)
                addr2_emb = torch.tensor([self.train_embeddings[pair[1]] for pair in batch_pairs], dtype=torch.float32).to(self.device)
                labels = torch.zeros(len(batch_pairs), dtype=torch.float32).to(self.device)
                
                # Forward pass
                out1, out2 = self.siamese_model(addr1_emb, addr2_emb)
                loss = criterion(out1, out2, labels)
                
                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.siamese_model.parameters(), max_norm=1.0)
                optimizer.step()
                
                total_loss += loss.item()
                batch_count += 1
            
            avg_loss = total_loss / batch_count if batch_count > 0 else float('inf')
            scheduler.step()
            
            print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}")
            
            # Early stopping
            if avg_loss < best_loss:
                best_loss = avg_loss
                patience_counter = 0
            else:
                patience_counter += 1
                
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch+1}")
                break
        
        print("âœ… Siamese network eÄŸitimi tamamlandÄ±")
    
    def _create_training_pairs(self):
        """GeliÅŸtirilmiÅŸ eÄŸitim iÃ§in veri Ã§iftleri oluÅŸtur"""
        positive_pairs = []
        negative_pairs = []
        
        # Label gruplarÄ±
        label_groups = {}
        for i, label in enumerate(self.train_labels):
            if label not in label_groups:
                label_groups[label] = []
            label_groups[label].append(i)
        
        # Positive pairs (aynÄ± label)
        for label, indices in label_groups.items():
            if len(indices) >= 2:
                for i in range(len(indices)):
                    for j in range(i+1, min(i+5, len(indices))):  # Her adres iÃ§in max 4 positive pair
                        positive_pairs.append((indices[i], indices[j]))
        
        # Negative pairs (farklÄ± label)
        labels = list(label_groups.keys())
        for i in range(min(len(positive_pairs), 2000)):  # Daha fazla negative pair
            label1, label2 = np.random.choice(labels, 2, replace=False)
            idx1 = np.random.choice(label_groups[label1])
            idx2 = np.random.choice(label_groups[label2])
            negative_pairs.append((idx1, idx2))
        
        print(f"ðŸ“Š Veri Ã§iftleri: {len(positive_pairs)} positive, {len(negative_pairs)} negative")
        return positive_pairs, negative_pairs
    
    def find_similar_addresses(self, query_address, top_k=5, method='ensemble'):
        """GeliÅŸtirilmiÅŸ benzer adresleri bul"""
        if self.embedding_model is None:
            raise ValueError("Model henÃ¼z eÄŸitilmemiÅŸ!")
        
        # Adres temizleme
        cleaned_query = self.preprocessor.clean_address(query_address)
        
        if method == 'ensemble' and self.ensemble_model:
            results = self.ensemble_model.predict(cleaned_query, top_k=top_k)
        else:
            # Sentence Transformers ile
            results = self.embedding_model.find_similar_advanced(
                cleaned_query, 
                self.train_embeddings, 
                self.train_labels, 
                top_k,
                method='cosine'
            )
        
        # Adres bilgilerini ekle
        for result in results:
            label = result['label']
            # Label'a karÅŸÄ±lÄ±k gelen adresleri bul
            matching_indices = [i for i, l in enumerate(self.train_labels) if l == label]
            if matching_indices:
                result['address'] = self.train_addresses[matching_indices[0]]
        
        return results
    
    def predict_label(self, query_address, method='ensemble'):
        """GeliÅŸtirilmiÅŸ label tahmini - TÃœM OPTÄ°MÄ°ZASYONLAR"""
        if method == 'hierarchical' and self.hierarchical_classifier:
            return self._hierarchical_predict(query_address)
        elif method == 'distilled' and self.distilled_model:
            return self._distilled_predict(query_address)
        elif method == 'multi_task' and self.multi_task_model:
            return self._multi_task_predict(query_address)
        elif method == 'ensemble' and self.ensemble_model:
            return self._ensemble_predict(query_address)
        elif method == 'fuzzy':
            # Fuzzy string matching
            cleaned_query = self.preprocessor.clean_address(query_address)
            best_match = process.extractOne(cleaned_query, self.train_addresses, scorer=fuzz.token_sort_ratio)
            if best_match and best_match[1] > 80:  # %80 Ã¼zeri benzerlik
                idx = self.train_addresses.index(best_match[0])
                return self.train_labels[idx]
            return None
        elif method == 'random_forest' and self.rf_classifier:
            # Random Forest tahmini
            cleaned_query = self.preprocessor.clean_address(query_address)
            tfidf_features = self.tfidf_vectorizer.transform([cleaned_query])
            predicted_encoded = self.rf_classifier.predict(tfidf_features)[0]
            return self.label_encoder.inverse_transform([predicted_encoded])[0]
        else:
            # Default: similarity
            return self._similarity_predict(query_address)
    
    def _hierarchical_predict(self, query_address):
        """Hierarchical tahmin"""
        # 1. BÃ¶lge tahmini
        region_features = self.hierarchical_classifier.extract_region_features(query_address)
        # Basit region belirleme
        if any(region_features.values()):
            region = 'urban'
        else:
            region = 'rural'
        
        # 2. O bÃ¶lgedeki adresler arasÄ±ndan tahmin
        region_labels = self.hierarchical_classifier.region_mapping.get(region, [])
        if not region_labels:
            return self._similarity_predict(query_address)
        
        # Sadece o bÃ¶lgedeki embedding'leri kullan
        region_indices = [i for i, label in enumerate(self.train_labels) if label in region_labels]
        if not region_indices:
            return self._similarity_predict(query_address)
        
        region_embeddings = self.train_embeddings[region_indices]
        region_train_labels = [self.train_labels[i] for i in region_indices]
        
        # Similarity hesapla
        query_embedding = self.embedding_model.model.encode([query_address])
        similarities = cosine_similarity(query_embedding, region_embeddings)[0]
        best_idx = np.argmax(similarities)
        
        return region_train_labels[best_idx]
    
    def _distilled_predict(self, query_address):
        """Distilled model tahmini"""
        query_embedding = self.embedding_model.model.encode([query_address])
        query_tensor = torch.tensor(query_embedding, dtype=torch.float32).to(self.device)
        
        self.distilled_model.eval()
        with torch.no_grad():
            logits = self.distilled_model(query_tensor)
            predicted_idx = torch.argmax(logits, dim=1).item()
        
        return self.label_encoder.inverse_transform([predicted_idx])[0]
    
    def _multi_task_predict(self, query_address):
        """Multi-task model tahmini"""
        query_embedding = self.embedding_model.model.encode([query_address])
        query_tensor = torch.tensor(query_embedding, dtype=torch.float32).to(self.device)
        
        self.multi_task_model.eval()
        with torch.no_grad():
            main_logits, _, _ = self.multi_task_model(query_tensor)
            predicted_idx = torch.argmax(main_logits, dim=1).item()
        
        return self.label_encoder.inverse_transform([predicted_idx])[0]
    
    def _ensemble_predict(self, query_address):
        """Advanced ensemble tahmin"""
        predictions = {}
        weights = {}
        
        # Similarity prediction
        try:
            similar_result = self._similarity_predict(query_address)
            predictions['similarity'] = similar_result
            weights['similarity'] = 1.0
        except:
            pass
        
        # Distilled prediction
        if self.distilled_model:
            try:
                distilled_result = self._distilled_predict(query_address)
                predictions['distilled'] = distilled_result
                weights['distilled'] = 0.8
            except:
                pass
        
        # Multi-task prediction
        if self.multi_task_model:
            try:
                multitask_result = self._multi_task_predict(query_address)
                predictions['multi_task'] = multitask_result
                weights['multi_task'] = 0.9
            except:
                pass
        
        # Hierarchical prediction
        if self.hierarchical_classifier:
            try:
                hierarchical_result = self._hierarchical_predict(query_address)
                predictions['hierarchical'] = hierarchical_result
                weights['hierarchical'] = 0.7
            except:
                pass
        
        # Weighted voting
        if predictions:
            vote_counts = {}
            for method, prediction in predictions.items():
                weight = weights.get(method, 1.0)
                if prediction not in vote_counts:
                    vote_counts[prediction] = 0
                vote_counts[prediction] += weight
            
            # En yÃ¼ksek oy alan tahmini dÃ¶ndÃ¼r
            best_prediction = max(vote_counts.items(), key=lambda x: x[1])[0]
            return best_prediction
        
        # Fallback
        return self._similarity_predict(query_address)
    
    def _similarity_predict(self, query_address):
        """Basit similarity tahmini"""
        query_embedding = self.embedding_model.model.encode([query_address])
        similarities = cosine_similarity(query_embedding, self.train_embeddings)[0]
        best_idx = np.argmax(similarities)
        return self.train_labels[best_idx]
    
    def predict_batch(self, addresses, batch_size=100, method='ensemble'):
        """Toplu tahmin"""
        print(f"âš¡ Toplu tahmin baÅŸlÄ±yor: {len(addresses)} adres")
        
        all_predictions = []
        
        for i in tqdm(range(0, len(addresses), batch_size), desc="Batch prediction"):
            batch_addresses = addresses[i:i+batch_size]
            batch_predictions = []
            
            for address in batch_addresses:
                try:
                    predicted_label = self.predict_label(address, method=method)
                    batch_predictions.append(predicted_label)
                except Exception as e:
                    print(f"âš ï¸ Tahmin hatasÄ±: {e}")
                    batch_predictions.append(None)
            
            all_predictions.extend(batch_predictions)
        
        return all_predictions
    
    def save_model(self, filepath='models/bert_address_model.pkl'):
        """UNIFIED MODEL KAYDETME"""
        print(f"ðŸ’¾ Unified model kaydediliyor: {filepath}")
        
        model_data = {
            'train_embeddings': self.train_embeddings,
            'train_labels': self.train_labels,
            'train_addresses': self.train_addresses,
            'label_encoder': self.label_encoder,
            'tfidf_vectorizer': self.tfidf_vectorizer,
            'rf_classifier': self.rf_classifier,
            'preprocessor': self.preprocessor,
            'device': str(self.device)
        }
        
        # Unified model'i kaydet
        if self.unified_model:
            unified_path = filepath.replace('.pkl', '_unified.pth')
            torch.save(self.unified_model.state_dict(), unified_path)
            model_data['unified_model_path'] = unified_path
        
        joblib.dump(model_data, filepath)
        
        print(f"âœ… Unified model baÅŸarÄ±yla kaydedildi: {filepath}")
    
    @classmethod
    def load_model(cls, filepath='models/bert_address_model.pkl'):
        """UNIFIED MODEL YÃœKLEME"""
        print(f"ðŸ“‚ Unified model yÃ¼kleniyor: {filepath}")
        
        model_data = joblib.load(filepath)
        
        # Model nesnesini oluÅŸtur
        device = torch.device(model_data.get('device', 'cpu'))
        model = cls(device=device)
        
        model.embedding_model = MemoryEfficientEmbeddingModel(device=device)
        model.train_embeddings = model_data['train_embeddings']
        model.train_labels = model_data['train_labels']
        model.train_addresses = model_data['train_addresses']
        model.label_encoder = model_data.get('label_encoder')
        model.tfidf_vectorizer = model_data.get('tfidf_vectorizer')
        model.rf_classifier = model_data.get('rf_classifier')
        model.preprocessor = model_data.get('preprocessor', AddressPreprocessor())
        
        # Unified model'i yÃ¼kle
        if 'unified_model_path' in model_data:
            try:
                num_labels = len(model.label_encoder.classes_) if model.label_encoder else 10390
                model.unified_model = UnifiedAddressModel(
                    embedding_dim=384,
                    num_labels=num_labels
                ).to(device)
                model.unified_model.load_state_dict(torch.load(model_data['unified_model_path'], map_location=device))
                model.unified_model.eval()
                print("âœ… Unified model yÃ¼klendi")
            except Exception as e:
                print(f"âš ï¸ Unified model yÃ¼kleme hatasÄ±: {e}")
        
        # Lightweight ensemble model oluÅŸtur (backup iÃ§in)
        model.ensemble_model = EnsembleAddressMatcher()
        model.ensemble_model.add_model('sentence_transformer', model, weight=1.0)
        if model.rf_classifier:
            model.ensemble_model.add_model('random_forest', model, weight=0.7)
        
        print(f"âœ… Unified model baÅŸarÄ±yla yÃ¼klendi!")
        return model

def train_unified_bert_model():
    """UNIFIED BERT modelini eÄŸit - TÃœM OPTÄ°MÄ°ZASYONLAR BÄ°RLEÅžTÄ°RÄ°LDÄ°"""
    print("ðŸš€ UNIFIED BERT ADRES EÅžLEÅžTÄ°RME MODELÄ° EÄžÄ°TÄ°MÄ°")
    print("=" * 80)
    print("ðŸŽ¯ UNIFIED MODEL - TEK MODEL, TÃœM OPTÄ°MÄ°ZASYONLAR BÄ°RLÄ°KTE:")
    print("  âœ… Memory Management (16GB RAM iÃ§in)")
    print("  âœ… Advanced Data Augmentation")
    print("  âœ… Multi-Task Learning (Main + Region + Type)")
    print("  âœ… Knowledge Distillation")
    print("  âœ… Contrastive Learning (Similarity)")
    print("  âœ… Feature Regularization")
    print("  âœ… Lightweight Ensemble (Backup)")
    print("=" * 80)
    
    # Train verisini yÃ¼kle
    print("ðŸ“Š Train verisi yÃ¼kleniyor...")
    try:
        train_df = pd.read_csv('data/train.csv')
        print(f"âœ… Train verisi yÃ¼klendi: {train_df.shape}")
    except Exception as e:
        print(f"âŒ Train verisi yÃ¼kleme hatasÄ±: {e}")
        return
    
    # Veri kontrolÃ¼
    print(f"\nðŸ“‹ Veri Analizi:")
    print(f"  - Toplam kayÄ±t: {len(train_df)}")
    print(f"  - Benzersiz label: {train_df['label'].nunique()}")
    print(f"  - Ortalama adres uzunluÄŸu: {train_df['address'].str.len().mean():.1f}")
    
    # Memory kontrolÃ¼
    MemoryManager.check_memory_safe()
    
    # Model oluÅŸtur ve eÄŸit
    print(f"\nðŸ”§ UNIFIED MODEL EÄžÄ°TÄ°MÄ°")
    print("=" * 80)
    
    model = OptimizedBERTAddressMatcher()
    model.fit(train_df, use_augmentation=True)
    
    # Modeli kaydet
    print(f"\nðŸ’¾ OPTÄ°MÄ°ZE EDÄ°LMÄ°Åž MODEL KAYDETME")
    print("=" * 80)
    
    import os
    os.makedirs('models', exist_ok=True)
    model.save_model('models/bert_address_model.pkl')
    
    # Test et
    print(f"\nðŸ§ª UNIFIED MODEL TEST")
    print("=" * 80)
    
    # Ã–rnek test
    test_address = train_df.iloc[0]['address']
    print(f"ðŸ” Test adresi: {test_address}")
    
    # Unified model test
    try:
        predicted_label = model.predict_label(test_address, method='unified')
        actual_label = train_df.iloc[0]['label']
        is_correct = predicted_label == actual_label
        status = "âœ…" if is_correct else "âŒ"
        print(f"{status} UNIFIED MODEL: {predicted_label} (GerÃ§ek: {actual_label})")
    except Exception as e:
        print(f"âš ï¸ UNIFIED MODEL hatasÄ±: {e}")
    
    # Backup yÃ¶ntemlerle test
    backup_methods = ['ensemble', 'fuzzy', 'random_forest', 'similarity']
    print(f"\nðŸ”„ Backup yÃ¶ntemlerle test:")
    for method in backup_methods:
        try:
            predicted_label = model.predict_label(test_address, method=method)
            actual_label = train_df.iloc[0]['label']
            is_correct = predicted_label == actual_label
            status = "âœ…" if is_correct else "âŒ"
            print(f"{status} {method.upper()}: {predicted_label} (GerÃ§ek: {actual_label})")
        except Exception as e:
            print(f"âš ï¸ {method.upper()} hatasÄ±: {e}")
    
    # Benzer adresleri bul
    similar_addresses = model.find_similar_addresses(test_address, top_k=3, method='ensemble')
    print(f"\nðŸ“‹ En benzer 3 adres (Unified Model):")
    for i, addr in enumerate(similar_addresses):
        print(f"  {i+1}. Label: {addr['label']}, Score: {addr.get('score', addr['similarity']):.4f}")
        if 'address' in addr:
            print(f"     Adres: {addr['address'][:80]}...")
    
    print(f"\nðŸŽ‰ UNIFIED BERT model eÄŸitimi baÅŸarÄ±yla tamamlandÄ±!")
    print(f"ðŸ“ Model dosyasÄ±: models/bert_address_model.pkl")
    print(f"ðŸ’¾ Memory kullanÄ±mÄ± optimize edildi (16GB RAM iÃ§in)")
    print(f"ðŸš€ TÃœM OPTÄ°MÄ°ZASYONLAR BÄ°RLEÅžTÄ°RÄ°LDÄ°!")
    print(f"ðŸŽ¯ TEK MODEL, MAKSÄ°MUM PERFORMANS!")
    print(f"ðŸ’¾ CHECKPOINT SÄ°STEMÄ° AKTÄ°F - GÃœVENLÄ° KAYDETME!")

def test_checkpoint_system():
    """Checkpoint sistemini test et"""
    print("ðŸ§ª CHECKPOINT SÄ°STEMÄ° TEST")
    print("=" * 50)
    
    # Checkpoint manager oluÅŸtur
    checkpoint_manager = CheckpointManager()
    
    # Test verisi
    test_data = {
        'test_embeddings': np.random.rand(100, 384),
        'test_addresses': ['test_address_1', 'test_address_2'],
        'test_labels': ['label_1', 'label_2']
    }
    
    test_metadata = {
        'test_type': 'checkpoint_test',
        'data_size': 100
    }
    
    # Checkpoint kaydet
    print("ðŸ“ Test checkpoint kaydediliyor...")
    success = checkpoint_manager.save_checkpoint('test_checkpoint', test_data, test_metadata)
    
    if success:
        print("âœ… Test checkpoint kaydedildi")
        
        # Checkpoint yÃ¼kle
        print("ðŸ“‚ Test checkpoint yÃ¼kleniyor...")
        loaded_data, loaded_metadata = checkpoint_manager.load_checkpoint('test_checkpoint')
        
        if loaded_data is not None:
            print("âœ… Test checkpoint yÃ¼klendi")
            print(f"ðŸ“Š YÃ¼klenen veri boyutu: {loaded_data['test_embeddings'].shape}")
            print(f"ðŸ“… Metadata: {loaded_metadata}")
        else:
            print("âŒ Test checkpoint yÃ¼kleme hatasÄ±")
    else:
        print("âŒ Test checkpoint kaydetme hatasÄ±")
    
    # Mevcut checkpoint'leri listele
    print("\nðŸ“‹ Mevcut checkpoint'ler:")
    checkpoints = checkpoint_manager.list_checkpoints()
    for checkpoint in checkpoints:
        print(f"  - {checkpoint['name']}")
        if checkpoint['metadata']:
            print(f"    ðŸ“… {checkpoint['metadata'].get('timestamp', 'Bilinmiyor')}")
    
    print("\nâœ… Checkpoint sistemi test tamamlandÄ±!")

if __name__ == "__main__":
    # Ana model eÄŸitimi
    train_unified_bert_model()
